{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and setting seeds for reprodicibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import functools\n",
    "import torch\n",
    "import torchtext.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import UDPOS\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from transformers import BertModel,BertTokenizer\n",
    "\n",
    "\n",
    "from preprocessing import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 7\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#torch.cuda.manual_seed_all(SEED)\n",
    "device='cpu'\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "TRANSFORMER = \"bert-base-uncased\"\n",
    "# Setting up dataloaders for training.\n",
    "tokenizer = BertTokenizer.from_pretrained(TRANSFORMER) # tokenizer for BERT\n",
    "init_token = tokenizer.cls_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "sep_token = tokenizer.sep_token\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "sep_token_idx = tokenizer.convert_tokens_to_ids(sep_token)\n",
    "max_input_length = tokenizer.max_model_input_sizes[TRANSFORMER]\n",
    "\n",
    "\n",
    "\n",
    "train_datapipe = UDPOS(split=\"train\")\n",
    "valid_datapipe = UDPOS(split=\"valid\")\n",
    "pos_vocab = build_vocab_from_iterator(\n",
    "    [i[1] for i in list(train_datapipe)],\n",
    "    specials=[init_token, pad_token, sep_token],\n",
    ")\n",
    "T_CAL = torch.tensor([i for i in range(pos_vocab.__len__())])\n",
    "\n",
    "text_preprocessor = functools.partial(\n",
    "    prepare_words,\n",
    "    tokenizer=tokenizer,\n",
    "    max_input_length=max_input_length,\n",
    "    init_token=init_token,\n",
    "    sep_token=sep_token,\n",
    ")\n",
    "\n",
    "tag_preprocessor = functools.partial(\n",
    "    prepare_tags,\n",
    "    max_input_length=max_input_length,\n",
    "    init_token=init_token,\n",
    "    sep_token=sep_token,\n",
    "    pos_vocab=pos_vocab\n",
    ")\n",
    "\n",
    "\n",
    "def apply_transform(x):\n",
    "    return text_preprocessor(x[0]), tag_preprocessor(x[1])\n",
    "\n",
    "\n",
    "train_datapipe = (\n",
    "    train_datapipe.map(apply_transform)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .rows2columnar([\"words\", \"pos\"])\n",
    ")\n",
    "train_dataloader = DataLoader(train_datapipe, batch_size=None, shuffle=False)\n",
    "\n",
    "valid_datapipe = (\n",
    "    valid_datapipe.map(apply_transform)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .rows2columnar([\"words\", \"pos\"])\n",
    ")\n",
    "valid_dataloader = DataLoader(valid_datapipe, batch_size=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta : 0.0 \n",
      "\n",
      "Epoch: 1 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [26:13<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Development set accuracy: 0.8823398351669312\n",
      "-------------------------\n",
      "Epoch: 2 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [25:07<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Development set accuracy: 0.9094575643539429\n",
      "-------------------------\n",
      "Epoch: 3 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [23:58<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Development set accuracy: 0.9199662208557129\n",
      "-------------------------\n",
      "beta : 0.1 \n",
      "\n",
      "Epoch: 1 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [33:50<00:00,  5.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Development set accuracy: 0.8871060609817505\n",
      "-------------------------\n",
      "Epoch: 2 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [33:51<00:00,  5.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Development set accuracy: 0.9016318321228027\n",
      "-------------------------\n",
      "Epoch: 3 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [33:51<00:00,  5.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Development set accuracy: 0.9221358299255371\n",
      "-------------------------\n",
      "beta : 1 \n",
      "\n",
      "Epoch: 1 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [33:54<00:00,  5.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Development set accuracy: 0.8996726274490356\n",
      "-------------------------\n",
      "Epoch: 2 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [5:30:14<00:00, 50.55s/it]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Development set accuracy: 0.9156255125999451\n",
      "-------------------------\n",
      "Epoch: 3 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [34:36<00:00,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Development set accuracy: 0.9252203702926636\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "\n",
    "best_acc = 0.0\n",
    "betas = [0.0,0.1,1]\n",
    "for beta in betas:\n",
    "    print('beta : {} \\n'.format(beta))\n",
    "    torch.manual_seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    bert = BertModel.from_pretrained(TRANSFORMER)\n",
    "    crf = NeuralCRF(\n",
    "        pad_idx_word=pad_token_idx,\n",
    "        pad_idx_pos=pos_vocab[pad_token],\n",
    "        bos_idx=init_token_idx,\n",
    "        eos_idx=sep_token_idx,\n",
    "        bot_idx=pos_vocab[init_token],\n",
    "        eot_idx=pos_vocab[sep_token],\n",
    "        t_cal=T_CAL,\n",
    "        transformer=bert,\n",
    "        beta=beta\n",
    "    )\n",
    "    if device!= 'cpu' : torch.cuda.empty_cache()\n",
    "    crf.to(device)\n",
    "    accuracy = train_model_report_accuracy(\n",
    "        crf,\n",
    "        LR,\n",
    "        EPOCHS,\n",
    "        train_dataloader,\n",
    "        valid_dataloader,\n",
    "        pad_token_idx,\n",
    "        pos_vocab[pad_token],\n",
    "        device\n",
    "    )\n",
    "    if accuracy > best_acc:\n",
    "        best_acc = accuracy\n",
    "        best_model = crf\n",
    "        best_beta = beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"pos_model.pt\"\n",
    "torch.save(best_model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
